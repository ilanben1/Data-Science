{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "#nltk.download()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_in_file(a, b):\n",
    "    for x in os.listdir():\n",
    "        if x.endswith('.txt'):\n",
    "            if b in x:\n",
    "                print(x)\n",
    "                with open(x,'w') as f:\n",
    "                    f.write(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet_articlei_1 = open('internet_article_1.txt','r').read()\n",
    "list_sentences_internet_1 = tokenizer.tokenize(internet_article_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet_article_2 = open('internet_article_2.txt','r').read()\n",
    "list_sentences_internet_2 = tokenizer.tokenize(internet_article_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_article = open('sport_article.txt','r').read()\n",
    "sport_article = tokenizer.tokenize(sport_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "def no_stop(list_sentences):\n",
    "    list_final = []\n",
    "    for sentences in list_sentences:\n",
    "        list_words = tokenizer.tokenize(sentences)\n",
    "        words = [word for word in list_words if word not in stop]\n",
    "        list_final.append(' '.join(words))\n",
    "    return list_final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_final_1 = no_stop(list_sentences_internet_1)\n",
    "list_final_2 = no_stop(list_sentences_internet_2)\n",
    "list_final_sport = no_stop(sport_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import *\n",
    "def compute_dist(list_A, list_B):\n",
    "    dist = (1,0)\n",
    "    for x in range(len(list_A)):\n",
    "        A = set(list_A[x])\n",
    "        for y in range(len(list_B)):\n",
    "            B = set(list_B[y])\n",
    "            dist_j = jaccard_distance(A,B) \n",
    "            if dist_j < dist[0]:\n",
    "                dist = (jaccard_distance(A,B),[x,y])\n",
    "    print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5, [0, 95])\n"
     ]
    }
   ],
   "source": [
    "compute_dist(list_final_1, list_final_sport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'replacers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fadf64a95a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreplacers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpReplacer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreplacer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRegexpReplacer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreplacer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minternet_article_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'replacers'"
     ]
    }
   ],
   "source": [
    "from replacers import RegexpReplacer \n",
    "replacer=RegexpReplacer()\n",
    "replacer.replace(internet_article_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'internet',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'built',\n",
       " 'for',\n",
       " 'kids',\n",
       " 'In',\n",
       " '1969',\n",
       " ',',\n",
       " 'children',\n",
       " '’',\n",
       " 's',\n",
       " 'entertainer',\n",
       " 'Fred',\n",
       " 'Rogers',\n",
       " 'appeared',\n",
       " 'before',\n",
       " 'the',\n",
       " 'U.S.',\n",
       " 'Senate',\n",
       " 'and',\n",
       " 'voiced',\n",
       " '“',\n",
       " 'concern',\n",
       " 'about',\n",
       " 'what',\n",
       " 'our',\n",
       " 'children',\n",
       " 'are',\n",
       " 'seeing',\n",
       " '”',\n",
       " 'on',\n",
       " 'television.',\n",
       " 'He',\n",
       " 'emphasized',\n",
       " 'the',\n",
       " 'transformative',\n",
       " 'potential',\n",
       " 'of',\n",
       " 'television',\n",
       " 'and',\n",
       " 'insisted',\n",
       " 'that',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'appropriate',\n",
       " 'and',\n",
       " 'thoughtful',\n",
       " 'children',\n",
       " '’',\n",
       " 's',\n",
       " 'entertainment',\n",
       " 'required',\n",
       " 'a',\n",
       " '“',\n",
       " 'meaningful',\n",
       " 'expression',\n",
       " 'of',\n",
       " 'care.',\n",
       " '”',\n",
       " 'The',\n",
       " 'world',\n",
       " 'of',\n",
       " 'children',\n",
       " '’',\n",
       " 's',\n",
       " 'media',\n",
       " 'has',\n",
       " 'come',\n",
       " 'a',\n",
       " 'long',\n",
       " 'way',\n",
       " 'since',\n",
       " 'then.',\n",
       " 'But',\n",
       " 'it',\n",
       " 'feels',\n",
       " 'like',\n",
       " 'we',\n",
       " '’',\n",
       " 've',\n",
       " 'strayed',\n",
       " 'pretty',\n",
       " 'far',\n",
       " 'from',\n",
       " 'the',\n",
       " 'path',\n",
       " 'that',\n",
       " 'Mr.',\n",
       " 'Rogers',\n",
       " 'laid',\n",
       " 'out',\n",
       " 'for',\n",
       " 'us.',\n",
       " 'YouTube',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'news',\n",
       " 'again',\n",
       " ',',\n",
       " 'this',\n",
       " 'time',\n",
       " 'after',\n",
       " 'being',\n",
       " 'hit',\n",
       " 'with',\n",
       " 'a',\n",
       " 'record',\n",
       " '$',\n",
       " '170',\n",
       " 'million',\n",
       " 'fine',\n",
       " 'in',\n",
       " 'a',\n",
       " 'settlement',\n",
       " 'with',\n",
       " 'the',\n",
       " 'FTC',\n",
       " 'for',\n",
       " 'violating',\n",
       " 'children',\n",
       " '’',\n",
       " 's',\n",
       " 'privacy',\n",
       " 'regulations.',\n",
       " 'For',\n",
       " 'months',\n",
       " ',',\n",
       " 'we',\n",
       " '’',\n",
       " 've',\n",
       " 'seen',\n",
       " 'dozens',\n",
       " 'of',\n",
       " 'stories',\n",
       " 'about',\n",
       " 'YouTube',\n",
       " ',',\n",
       " 'each',\n",
       " 'detailing',\n",
       " 'disturbing',\n",
       " 'and',\n",
       " 'potentially',\n",
       " 'harmful',\n",
       " 'content',\n",
       " 'being',\n",
       " 'served',\n",
       " 'up',\n",
       " 'to',\n",
       " 'children.',\n",
       " 'But',\n",
       " 'now',\n",
       " 'the',\n",
       " 'conversation',\n",
       " 'has',\n",
       " 'changed.',\n",
       " 'Most',\n",
       " 'agree',\n",
       " 'that',\n",
       " 'YouTube',\n",
       " 'isn',\n",
       " '’',\n",
       " 't',\n",
       " 'a',\n",
       " 'safe',\n",
       " 'place',\n",
       " 'for',\n",
       " 'children',\n",
       " 'to',\n",
       " 'roam',\n",
       " 'free.',\n",
       " 'The',\n",
       " 'question',\n",
       " 'isn',\n",
       " '’',\n",
       " 't',\n",
       " 'whether',\n",
       " 'or',\n",
       " 'not',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'safe',\n",
       " 'for',\n",
       " 'kids',\n",
       " ',',\n",
       " 'but',\n",
       " 'if',\n",
       " 'it',\n",
       " 'can',\n",
       " 'ever',\n",
       " 'be',\n",
       " 'safe.',\n",
       " 'There',\n",
       " 'has',\n",
       " 'been',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'speculation',\n",
       " 'about',\n",
       " 'where',\n",
       " 'YouTube',\n",
       " 'should',\n",
       " 'go',\n",
       " 'from',\n",
       " 'here',\n",
       " ',',\n",
       " 'but',\n",
       " 'no',\n",
       " 'one',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'agree',\n",
       " 'on',\n",
       " 'the',\n",
       " 'best',\n",
       " 'course',\n",
       " 'of',\n",
       " 'action.',\n",
       " 'To',\n",
       " 'quote',\n",
       " 'CNN',\n",
       " '’',\n",
       " 's',\n",
       " 'Brian',\n",
       " 'Stelter',\n",
       " 'on',\n",
       " 'the',\n",
       " 'matter',\n",
       " ',',\n",
       " '“',\n",
       " 'Every',\n",
       " 'suggested',\n",
       " 'solution',\n",
       " 'leads',\n",
       " 'to',\n",
       " 'more',\n",
       " 'questions…',\n",
       " '”']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(internet_article_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
    "similarity_message_encodings = embed(list_sentences_internet_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0392116  0.5426129  0.5185502  0.60741866 0.23267345 0.24638376\n",
      "  0.37782902 0.702317   0.53949404 0.05340255]\n",
      " [0.5426129  2.2163918  0.8584224  1.0233926  0.6650567  0.5046911\n",
      "  0.36911038 0.5964512  0.95206916 0.24566808]\n",
      " [0.5185502  0.8584224  1.709933   0.72202975 0.29053015 0.64992994\n",
      "  0.43818042 0.9752233  1.0351375  0.26842743]\n",
      " [0.60741866 1.0233926  0.72202975 1.9407623  0.3587065  0.7013468\n",
      "  0.3671206  0.5077011  0.86597943 0.3154996 ]\n",
      " [0.23267345 0.6650567  0.29053015 0.3587065  1.3471142  0.33246094\n",
      "  0.31032604 0.41210857 0.8617826  0.1635743 ]\n",
      " [0.24638376 0.5046911  0.64992994 0.7013468  0.33246094 1.018465\n",
      "  0.24084121 0.5366736  0.7053482  0.15912056]\n",
      " [0.37782902 0.36911038 0.43818042 0.3671206  0.31032604 0.24084121\n",
      "  0.82147545 0.67224276 0.43019366 0.22216615]\n",
      " [0.702317   0.5964512  0.9752233  0.5077011  0.41210857 0.5366736\n",
      "  0.67224276 1.7296981  0.6427838  0.16193208]\n",
      " [0.53949404 0.95206916 1.0351375  0.86597943 0.8617826  0.7053482\n",
      "  0.43019366 0.6427838  1.8609743  0.4189834 ]\n",
      " [0.05340255 0.24566808 0.26842743 0.3154996  0.1635743  0.15912056\n",
      "  0.22216615 0.16193208 0.4189834  1.099563  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    message_embeddings_ = session.run(similarity_message_encodings, feed_dict={similarity_input_placeholder: list_sentences_internet_1})\n",
    "\n",
    "    corr = np.inner(message_embeddings_, message_embeddings_)\n",
    "    print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "embed = hub.Module(\"https://tfhub.dev/google/nnlm-en-dim50/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed([\"The movie was great!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.eval of <tf.Tensor 'module_apply_default/embedding_lookup_sparse:0' shape=(?, 50) dtype=float32>>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
